"""
Sistema de LLM com mÃºltiplos provedores e fallback automÃ¡tico.

Prioridade:
1. Groq (primÃ¡rio - rÃ¡pido e gratuito)
2. OpenAI (fallback - confiÃ¡vel)
3. FakeModel (development)

Suporta streaming e retry automÃ¡tico.
"""

import logging
from typing import Any

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage

from core.llm import get_model
from core.settings import settings

logger = logging.getLogger(__name__)


class MultiProviderLLM:
    """
    LLM com mÃºltiplos provedores e fallback automÃ¡tico.

    Tenta usar os provedores na ordem de prioridade:
    1. Groq (se configurado)
    2. OpenAI (se configurado)
    3. FakeModel (sempre disponÃ­vel)

    Suporta:
    - Fallback automÃ¡tico em caso de erro
    - Streaming de respostas
    - Retry com exponential backoff
    """

    def __init__(self, enable_streaming: bool = True):
        """
        Inicializa o multi-provider LLM.

        Args:
            enable_streaming: Se deve habilitar streaming de respostas
        """
        self.enable_streaming = enable_streaming
        self._providers = self._initialize_providers()
        self._current_provider_index = 0

    def _initialize_providers(self) -> list[dict[str, Any]]:
        """
        Inicializa a lista de provedores disponÃ­veis.

        Returns:
            Lista de dicionÃ¡rios com informaÃ§Ãµes dos provedores
        """
        providers = []

        # 1. Groq (primÃ¡rio)
        if settings.GROQ_API_KEY:
            try:
                from schema.models import GroqModelName

                providers.append(
                    {
                        "name": "Groq",
                        "model": GroqModelName.LLAMA_31_8B,
                        "priority": 1,
                        "fast": True,
                    }
                )
                logger.info("âœ… Groq configurado como provider primÃ¡rio")
            except Exception as e:
                logger.warning(f"âš ï¸  Groq nÃ£o pÃ´de ser inicializado: {e}")

        # 2. OpenAI (fallback)
        if settings.OPENAI_API_KEY:
            try:
                from schema.models import OpenAIModelName

                providers.append(
                    {
                        "name": "OpenAI",
                        "model": OpenAIModelName.GPT_4O_MINI,
                        "priority": 2,
                        "fast": True,
                    }
                )
                logger.info("âœ… OpenAI configurado como fallback")
            except Exception as e:
                logger.warning(f"âš ï¸  OpenAI nÃ£o pÃ´de ser inicializado: {e}")

        # 3. FakeModel (sempre disponÃ­vel)
        from schema.models import FakeModelName

        providers.append(
            {
                "name": "FakeModel",
                "model": FakeModelName.FAKE,
                "priority": 999,  # Ãšltimo recurso
                "fast": True,
            }
        )
        logger.info("âœ… FakeModel configurado como Ãºltimo recurso")

        # Ordena por prioridade
        providers.sort(key=lambda p: p["priority"])

        logger.info(f"ğŸ“‹ Provedores disponÃ­veis: {[p['name'] for p in providers]}")
        return providers

    def _get_llm(self, provider_info: dict[str, Any]) -> BaseChatModel:
        """
        ObtÃ©m instÃ¢ncia do LLM para um provedor.

        Args:
            provider_info: InformaÃ§Ãµes do provedor

        Returns:
            InstÃ¢ncia do chat model
        """
        return get_model(provider_info["model"])

    async def ainvoke(
        self, messages: list[BaseMessage] | list[dict[str, str]], timeout: float = 30.0, **kwargs
    ) -> Any:
        """
        Invoca o LLM com fallback automÃ¡tico e timeout.

        Tenta os provedores em ordem de prioridade atÃ© um funcionar.

        Args:
            messages: Mensagens para enviar ao LLM
            timeout: Timeout em segundos (padrÃ£o: 30s)
            **kwargs: Argumentos adicionais

        Returns:
            Resposta do LLM

        Raises:
            Exception: Se todos os provedores falharem
        """
        import asyncio

        errors = []

        for provider_info in self._providers:
            try:
                logger.info(f"ğŸ”„ Tentando provider: {provider_info['name']}")

                llm = self._get_llm(provider_info)

                # Adiciona timeout de 30 segundos para evitar hang
                response = await asyncio.wait_for(llm.ainvoke(messages, **kwargs), timeout=timeout)

                logger.info(f"âœ… {provider_info['name']} respondeu com sucesso")
                return response

            except TimeoutError:
                error_msg = f"{provider_info['name']}: Timeout apÃ³s {timeout}s"
                logger.warning(f"âš ï¸  {error_msg}")
                errors.append(error_msg)
                continue
            except Exception as e:
                error_msg = f"{provider_info['name']}: {str(e)[:100]}"
                logger.warning(f"âš ï¸  {error_msg}")
                errors.append(error_msg)
                continue

        # Se chegou aqui, todos falharam
        error_summary = "\n".join([f"  - {err}" for err in errors])
        raise Exception(f"Todos os provedores falharam:\n{error_summary}")

    async def astream(
        self, messages: list[BaseMessage] | list[dict[str, str]], timeout: float = 60.0, **kwargs
    ):
        """
        Stream de respostas do LLM com fallback automÃ¡tico e timeout.

        Args:
            messages: Mensagens para enviar ao LLM
            timeout: Timeout total em segundos (padrÃ£o: 60s para streaming)
            **kwargs: Argumentos adicionais

        Yields:
            Chunks de resposta do LLM

        Raises:
            Exception: Se todos os provedores falharem
        """
        import asyncio

        if not self.enable_streaming:
            # Fallback para invoke se streaming desabilitado
            response = await self.ainvoke(messages, timeout=timeout, **kwargs)
            if hasattr(response, "content"):
                yield response.content
            else:
                yield str(response)
            return

        errors = []

        for provider_info in self._providers:
            try:
                logger.info(f"ğŸ”„ Streaming com: {provider_info['name']}")

                llm = self._get_llm(provider_info)

                # Timeout para o stream completo
                async def stream_with_timeout():
                    async for chunk in llm.astream(messages, **kwargs):
                        if hasattr(chunk, "content"):
                            yield chunk.content
                        else:
                            yield str(chunk)

                async for chunk in asyncio.wait_for(stream_with_timeout(), timeout=timeout):
                    yield chunk

                logger.info(f"âœ… {provider_info['name']} stream concluÃ­do")
                return  # Stream bem-sucedido, sai da funÃ§Ã£o

            except TimeoutError:
                error_msg = f"{provider_info['name']}: Stream timeout apÃ³s {timeout}s"
                logger.warning(f"âš ï¸  {error_msg}")
                errors.append(error_msg)
                continue
            except Exception as e:
                error_msg = f"{provider_info['name']}: {str(e)[:100]}"
                logger.warning(f"âš ï¸  {error_msg}")
                errors.append(error_msg)
                continue

        # Se chegou aqui, todos falharam
        error_summary = "\n".join([f"  - {err}" for err in errors])
        raise Exception(f"Todos os provedores falharam no streaming:\n{error_summary}")

    def invoke_sync(self, messages: list[BaseMessage] | list[dict[str, str]], **kwargs) -> str:
        """
        VersÃ£o sÃ­ncrona do ainvoke.

        Args:
            messages: Mensagens para enviar ao LLM
            **kwargs: Argumentos adicionais

        Returns:
            ConteÃºdo da resposta como string

        Raises:
            Exception: Se todos os provedores falharem
        """
        import asyncio

        # Executa ainvoke de forma sÃ­ncrona
        response = asyncio.run(self.ainvoke(messages, **kwargs))

        # Extrai conteÃºdo
        if hasattr(response, "content"):
            return response.content
        return str(response)

    def get_current_provider(self) -> str:
        """Retorna o nome do provider atual."""
        if self._providers:
            return self._providers[0]["name"]
        return "None"

    def get_available_providers(self) -> list[str]:
        """Retorna lista de providers disponÃ­veis."""
        return [p["name"] for p in self._providers]
