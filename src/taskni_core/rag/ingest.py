"""
Pipeline de ingest√£o de documentos para RAG.

Suporta:
- PDFs
- Arquivos de texto (.txt, .md)
- Chunking inteligente
- Embeddings com m√∫ltiplos provedores
- Armazenamento em ChromaDB
"""

import os
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime

from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# Embeddings
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import FakeEmbeddings, OllamaEmbeddings

from core.settings import settings
from taskni_core.core.settings import taskni_settings
from taskni_core.utils.security import sanitize_rag_filter

# Para detec√ß√£o de firewall
try:
    import httpx

    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False


class DocumentIngestion:
    """
    Pipeline de ingest√£o de documentos.

    Processa documentos (PDFs, textos), faz chunking,
    cria embeddings e armazena no ChromaDB.
    """

    def __init__(
        self,
        persist_directory: str = "./data/chroma",
        collection_name: str = "taskni_docs",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
    ):
        """
        Inicializa o pipeline de ingest√£o.

        Args:
            persist_directory: Diret√≥rio para persistir ChromaDB
            collection_name: Nome da cole√ß√£o no ChromaDB
            chunk_size: Tamanho dos chunks de texto
            chunk_overlap: Sobreposi√ß√£o entre chunks
        """
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # Cria diret√≥rio se n√£o existir
        os.makedirs(persist_directory, exist_ok=True)

        # Inicializa text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""],
        )

        # Inicializa embeddings
        self.embeddings = self._get_embeddings()

        # Inicializa vector store
        self.vectorstore = self._get_vectorstore()

    def _is_ollama_available(self) -> bool:
        """
        Detecta se o Ollama est√° dispon√≠vel e acess√≠vel.

        Returns:
            True se Ollama est√° acess√≠vel, False caso contr√°rio
        """
        if not taskni_settings.OLLAMA_BASE_URL:
            return False

        if not HTTPX_AVAILABLE:
            return False

        try:
            # Tenta acessar o endpoint /api/tags do Ollama
            base_url = taskni_settings.OLLAMA_BASE_URL.rstrip("/")
            with httpx.Client(
                timeout=3.0, verify=False
            ) as client:  # verify=False para HTTPS auto-assinado
                response = client.get(f"{base_url}/api/tags")
                return response.status_code == 200
        except Exception as e:
            print(f"‚ö†Ô∏è  Ollama n√£o acess√≠vel: {e}")
            return False

    def _is_firewalled(self) -> bool:
        """
        Detecta se o ambiente est√° atr√°s de firewall/proxy.

        Tenta acessar a API da OpenAI para verificar conectividade.

        Returns:
            True se est√° bloqueado, False se tem acesso
        """
        if not HTTPX_AVAILABLE:
            # Se httpx n√£o est√° dispon√≠vel, assume que est√° bloqueado
            return True

        try:
            # Tenta acessar endpoint da OpenAI com timeout curto
            with httpx.Client(timeout=2.0) as client:
                response = client.get("https://api.openai.com/v1/models")
                # Se chegou aqui, n√£o est√° bloqueado
                return False
        except Exception:
            # Qualquer erro (timeout, connection, SSL, etc) = bloqueado
            return True

    def _get_embeddings(self):
        """
        Retorna embeddings configurados com detec√ß√£o autom√°tica.

        Prioridade:
        1. Ollama (se configurado e acess√≠vel) - RECOMENDADO para produ√ß√£o
        2. OpenAI (se chave existe E ambiente n√£o est√° bloqueado)
        3. FakeEmbeddings (desenvolvimento ou quando h√° restri√ß√µes)

        Returns:
            Inst√¢ncia de embeddings configurada
        """
        # 1. PRIORIDADE: Ollama (embeddings locais/self-hosted)
        if taskni_settings.OLLAMA_BASE_URL:
            if self._is_ollama_available():
                try:
                    print(f"‚úÖ Usando Ollama Embeddings ({taskni_settings.OLLAMA_EMBED_MODEL})")
                    print(f"   Endpoint: {taskni_settings.OLLAMA_BASE_URL}")
                    return OllamaEmbeddings(
                        base_url=taskni_settings.OLLAMA_BASE_URL,
                        model=taskni_settings.OLLAMA_EMBED_MODEL,
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è  Ollama Embeddings falhou: {e}")
                    print("üìù Tentando fallback...")
            else:
                print(f"‚ö†Ô∏è  Ollama n√£o est√° acess√≠vel em {taskni_settings.OLLAMA_BASE_URL}")
                print("üìù Tentando fallback...")

        # 2. FALLBACK 1: OpenAI (se chave existe)
        if settings.OPENAI_API_KEY:
            # Detecta se ambiente est√° bloqueado
            is_blocked = self._is_firewalled()

            if not is_blocked:
                # Ambiente OK - usa OpenAI
                try:
                    print("‚úÖ Usando OpenAI Embeddings (text-embedding-3-small)")
                    return OpenAIEmbeddings(
                        api_key=settings.OPENAI_API_KEY.get_secret_value(),
                        model="text-embedding-3-small",  # Mais barato
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è  OpenAI Embeddings falhou: {e}")
                    print("üìù Fallback para FakeEmbeddings")
                    return FakeEmbeddings(size=768)  # nomic-embed-text usa 768 dims
            else:
                # Ambiente bloqueado
                print("‚ö†Ô∏è  Firewall/proxy detectado - acesso √† OpenAI bloqueado")
                print("üìù Usando FakeEmbeddings (desenvolvimento)")
                return FakeEmbeddings(size=768)

        # 3. FALLBACK FINAL: FakeEmbeddings
        print("‚ö†Ô∏è  Nenhum provedor de embeddings dispon√≠vel")
        print("üìù Usando FakeEmbeddings (desenvolvimento)")
        return FakeEmbeddings(size=768)

    def _get_vectorstore(self) -> Chroma:
        """Inicializa ou carrega o vector store ChromaDB."""
        return Chroma(
            collection_name=self.collection_name,
            embedding_function=self.embeddings,
            persist_directory=self.persist_directory,
        )

    def load_pdf(self, file_path: str) -> List[Document]:
        """
        Carrega e processa um arquivo PDF.

        Args:
            file_path: Caminho para o arquivo PDF

        Returns:
            Lista de documentos (chunks)
        """
        print(f"üìÑ Carregando PDF: {file_path}")

        loader = PyPDFLoader(file_path)
        documents = loader.load()

        print(f"   ‚úÖ {len(documents)} p√°ginas carregadas")

        # Chunking
        chunks = self.text_splitter.split_documents(documents)

        print(f"   ‚úÖ {len(chunks)} chunks criados")

        return chunks

    def load_text(self, file_path: str) -> List[Document]:
        """
        Carrega e processa um arquivo de texto.

        Args:
            file_path: Caminho para o arquivo de texto

        Returns:
            Lista de documentos (chunks)
        """
        print(f"üìù Carregando texto: {file_path}")

        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        print(f"   ‚úÖ Documento carregado")

        # Chunking
        chunks = self.text_splitter.split_documents(documents)

        print(f"   ‚úÖ {len(chunks)} chunks criados")

        return chunks

    def ingest_file(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> int:
        """
        Ingere um arquivo (PDF ou texto) no vector store.

        Args:
            file_path: Caminho para o arquivo
            metadata: Metadata adicional para os documentos

        Returns:
            N√∫mero de chunks ingeridos
        """
        file_extension = Path(file_path).suffix.lower()

        # Carrega documento baseado na extens√£o
        if file_extension == ".pdf":
            chunks = self.load_pdf(file_path)
        elif file_extension in [".txt", ".md"]:
            chunks = self.load_text(file_path)
        else:
            raise ValueError(f"Formato n√£o suportado: {file_extension}")

        # Adiciona metadata customizada
        if metadata:
            for chunk in chunks:
                chunk.metadata.update(metadata)

        # Adiciona metadata padr√£o
        for chunk in chunks:
            chunk.metadata["ingested_at"] = datetime.now().isoformat()
            chunk.metadata["source_file"] = os.path.basename(file_path)

        # Adiciona ao vector store
        print(f"üíæ Adicionando {len(chunks)} chunks ao ChromaDB...")
        self.vectorstore.add_documents(chunks)

        print(f"‚úÖ Ingest√£o completa: {len(chunks)} chunks")

        return len(chunks)

    def ingest_text_direct(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> int:
        """
        Ingere texto diretamente (sem arquivo).

        Args:
            text: Texto a ser ingerido
            metadata: Metadata para o documento

        Returns:
            N√∫mero de chunks ingeridos
        """
        print(f"üìù Ingerindo texto direto ({len(text)} caracteres)")

        # Cria documento
        doc = Document(page_content=text, metadata=metadata or {})

        # Chunking
        chunks = self.text_splitter.split_documents([doc])

        # Adiciona metadata padr√£o
        for chunk in chunks:
            chunk.metadata["ingested_at"] = datetime.now().isoformat()
            chunk.metadata["source"] = "direct_text"

        # Adiciona ao vector store
        print(f"üíæ Adicionando {len(chunks)} chunks ao ChromaDB...")
        self.vectorstore.add_documents(chunks)

        print(f"‚úÖ Ingest√£o completa: {len(chunks)} chunks")

        return len(chunks)

    def search(
        self, query: str, k: int = 4, filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        Busca documentos similares com sanitiza√ß√£o de filtros.

        Args:
            query: Texto de busca
            k: N√∫mero de documentos a retornar
            filter: Filtros de metadata (ser√° sanitizado)

        Returns:
            Lista de documentos mais relevantes
        """
        # SANITIZA FILTROS PARA PREVENIR SQL/NoSQL INJECTION
        if filter is not None:
            filter = sanitize_rag_filter(filter)

        results = self.vectorstore.similarity_search(query, k=k, filter=filter)

        return results

    def get_retriever(self, k: int = 4):
        """
        Retorna um retriever configurado.

        Args:
            k: N√∫mero de documentos a retornar

        Returns:
            Retriever do LangChain
        """
        return self.vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": k})

    def list_collections(self) -> List[str]:
        """Lista todas as cole√ß√µes no ChromaDB."""
        # ChromaDB pode ter m√∫ltiplas cole√ß√µes
        # Retorna a cole√ß√£o atual por enquanto
        return [self.collection_name]

    def get_collection_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas da cole√ß√£o."""
        collection = self.vectorstore._collection

        return {
            "name": self.collection_name,
            "count": collection.count(),
            "persist_directory": self.persist_directory,
        }

    def delete_collection(self):
        """Deleta a cole√ß√£o atual (cuidado!)."""
        print(f"üóëÔ∏è  Deletando cole√ß√£o: {self.collection_name}")
        self.vectorstore.delete_collection()
        print(f"‚úÖ Cole√ß√£o deletada")


# Inst√¢ncia global para uso no app
_ingestion_pipeline: Optional[DocumentIngestion] = None


def get_ingestion_pipeline() -> DocumentIngestion:
    """Retorna inst√¢ncia singleton do pipeline de ingest√£o."""
    global _ingestion_pipeline

    if _ingestion_pipeline is None:
        _ingestion_pipeline = DocumentIngestion()

    return _ingestion_pipeline
