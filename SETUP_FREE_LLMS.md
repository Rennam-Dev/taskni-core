# üÜì Configura√ß√£o de LLMs Gratuitas para o Taskni Core

Este guia mostra como configurar APIs de LLM gratuitas como backup para o Groq.

## üìã Op√ß√µes Recomendadas (em ordem de prioridade)

### 1. Google AI Studio (Gemini) ‚≠ê MELHOR OP√á√ÉO

**Por qu√™:**
- ‚úÖ Completamente gratuito
- ‚úÖ 15 requests/minuto (muito generoso)
- ‚úÖ Modelos excelentes (Gemini 2.0 Flash)
- ‚úÖ Sem necessidade de cart√£o de cr√©dito
- ‚úÖ Portugu√™s nativo

**Como obter a API key:**

1. Acesse: https://aistudio.google.com/apikey
2. Fa√ßa login com sua conta Google
3. Clique em "Create API Key"
4. Copie a chave gerada

**Configura√ß√£o no .env:**

```bash
# Google AI Studio (Gemini) - GRATUITO
GOOGLE_API_KEY=sua_chave_aqui
DEFAULT_MODEL=gemini-2.0-flash

# Alternativa: Gemini 1.5 Pro (mais poderoso, mesmo tier gratuito)
# DEFAULT_MODEL=gemini-1.5-pro
```

**Modelos dispon√≠veis:**
- `gemini-2.0-flash` - R√°pido, eficiente (recomendado)
- `gemini-2.0-flash-lite` - Ultra r√°pido, mais leve
- `gemini-1.5-pro` - Mais poderoso, context window maior

**Limites (tier gratuito):**
- 15 requests/minuto
- 1 milh√£o de tokens/m√™s
- 1500 requests/dia

---

### 2. Ollama (Local) ‚≠ê SEM LIMITES

**Por qu√™:**
- ‚úÖ 100% gratuito e privado
- ‚úÖ SEM limites de uso
- ‚úÖ Roda offline
- ‚úÖ V√°rios modelos dispon√≠veis
- ‚ö†Ô∏è  Precisa de recursos locais (RAM/CPU/GPU)

**Instala√ß√£o:**

```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# Windows
# Baixe de https://ollama.com/download
```

**Baixar modelos:**

```bash
# Modelos recomendados para produ√ß√£o
ollama pull llama3.2              # R√°pido, bom (3B)
ollama pull llama3.1:8b           # Balanceado (8B)
ollama pull mistral               # Excelente qualidade (7B)

# Modelos menores (para hardware limitado)
ollama pull phi3                  # Muito r√°pido (3.8B)
ollama pull gemma2:2b             # Ultra leve (2B)
```

**Configura√ß√£o no .env:**

```bash
# Ollama (Local)
OLLAMA_MODEL=llama3.2
DEFAULT_MODEL=ollama

# Se rodar Ollama em outro servidor
# OLLAMA_BASE_URL=http://seu-servidor:11434
```

**Iniciar Ollama:**

```bash
# Inicia o servidor Ollama
ollama serve

# Em outro terminal, teste
ollama run llama3.2 "Ol√° em portugu√™s"
```

---

### 3. OpenRouter ‚≠ê FALLBACK

**Por qu√™:**
- ‚úÖ Acesso a v√°rios modelos gratuitos
- ‚úÖ Fallback autom√°tico entre modelos
- ‚úÖ API unificada
- ‚ö†Ô∏è  Limites por modelo

**Como obter a API key:**

1. Acesse: https://openrouter.ai/keys
2. Crie uma conta (gratuita)
3. Gere uma API key

**Configura√ß√£o no .env:**

```bash
# OpenRouter
OPENROUTER_API_KEY=sua_chave_aqui
DEFAULT_MODEL=google/gemini-2.5-flash

# Outros modelos gratuitos dispon√≠veis:
# DEFAULT_MODEL=meta-llama/llama-3.2-3b-instruct:free
# DEFAULT_MODEL=google/gemini-flash-1.5:free
```

**Modelos gratuitos:**
- `google/gemini-2.5-flash` - Google Gemini (gratuito)
- `meta-llama/llama-3.2-3b-instruct:free` - Meta Llama
- `microsoft/phi-3-mini-128k-instruct:free` - Microsoft Phi-3

---

### 4. Hugging Face Inference API

**Por qu√™:**
- ‚úÖ Gratuito
- ‚úÖ Muitos modelos
- ‚ö†Ô∏è  Rate limits baixos
- ‚ö†Ô∏è  Pode ter cold start (lento)

**Como obter a API key:**

1. Acesse: https://huggingface.co/settings/tokens
2. Crie uma conta
3. Gere um Access Token

**Configura√ß√£o:**

```bash
# Hugging Face
HUGGINGFACE_API_KEY=hf_...
DEFAULT_MODEL=meta-llama/Llama-3.2-3B-Instruct
```

---

## üîÑ Sistema de Fallback Autom√°tico

Para m√°xima disponibilidade, voc√™ pode configurar m√∫ltiplas APIs:

```bash
# .env com fallback autom√°tico

# Op√ß√£o 1: Groq (quando voltar)
GROQ_API_KEY=gsk_...

# Op√ß√£o 2: Google Gemini (fallback principal)
GOOGLE_API_KEY=AI...

# Op√ß√£o 3: Ollama (fallback local)
OLLAMA_MODEL=llama3.2

# Op√ß√£o 4: OpenRouter (fallback final)
OPENROUTER_API_KEY=sk-or-...

# Prioridade de uso
DEFAULT_MODEL=llama-3.1-8b  # Tenta Groq primeiro
# Se Groq falhar, sistema tenta automaticamente Gemini, depois Ollama
```

---

## üí∞ Compara√ß√£o de Custos

| Provider | Custo | Limite Gratuito | Velocidade | Qualidade |
|----------|-------|-----------------|------------|-----------|
| **Gemini** | Gr√°tis | 1M tokens/m√™s | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Ollama** | Gr√°tis | Ilimitado* | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| **Groq** | Gr√°tis | Vari√°vel | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **OpenRouter** | Gr√°tis/Pago | Limitado | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| **HuggingFace** | Gr√°tis | Muito limitado | ‚ö° | ‚≠ê‚≠ê‚≠ê |

*Ilimitado mas depende do seu hardware

---

## üéØ Recomenda√ß√£o Final

**Para produ√ß√£o imediata (agora):**
```bash
GOOGLE_API_KEY=sua_chave_gemini
DEFAULT_MODEL=gemini-2.0-flash
```

**Para desenvolvimento (sem limites):**
```bash
OLLAMA_MODEL=llama3.2
DEFAULT_MODEL=ollama
```

**Para m√°xima disponibilidade (quando Groq voltar):**
```bash
# Tenta Groq primeiro, fallback para Gemini
GROQ_API_KEY=gsk_...
GOOGLE_API_KEY=AI...
DEFAULT_MODEL=llama-3.1-8b  # Groq
# Sistema automaticamente usa Gemini se Groq falhar
```

---

## üöÄ Pr√≥ximos Passos

1. **Escolha uma op√ß√£o acima**
2. **Obtenha a API key**
3. **Configure o .env**
4. **Reinicie o servidor**
5. **Teste com:** `python test_intake_scenarios.py`

---

## üìû Suporte

- Gemini: https://ai.google.dev/docs
- Ollama: https://ollama.com/docs
- OpenRouter: https://openrouter.ai/docs
- Groq: https://console.groq.com/docs
