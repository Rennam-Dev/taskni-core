#!/usr/bin/env python
"""
Teste do sistema multi-provedor com streaming.

Testa:
1. Fallback automÃ¡tico entre provedores (Groq â†’ OpenAI â†’ FakeModel)
2. Streaming de respostas
3. IntakeAgent com o novo sistema
"""

import sys
import asyncio

sys.path.insert(0, "/home/user/taskni-core/src")

from taskni_core.core.llm_provider import MultiProviderLLM
from taskni_core.agents.intake_agent import IntakeAgent

print("=" * 80)
print("ğŸ§ª TESTE DO SISTEMA MULTI-PROVEDOR COM STREAMING")
print("=" * 80)


async def test_multi_provider_direct():
    """Testa o MultiProviderLLM diretamente."""
    print("\n" + "=" * 80)
    print("ğŸ“‹ TESTE 1: MultiProviderLLM Direto (ainvoke)")
    print("=" * 80)

    llm = MultiProviderLLM(enable_streaming=False)

    print(f"\nâœ… Provedores disponÃ­veis: {llm.get_available_providers()}")
    print(f"ğŸ“ Provedor atual (primÃ¡rio): {llm.get_current_provider()}")

    messages = [
        {"role": "system", "content": "VocÃª Ã© um assistente amigÃ¡vel."},
        {"role": "user", "content": "Diga olÃ¡ em portuguÃªs de forma breve."},
    ]

    print(f"\nğŸ’¬ Enviando mensagem: 'Diga olÃ¡ em portuguÃªs de forma breve.'")

    try:
        response = await llm.ainvoke(messages)

        if hasattr(response, "content"):
            reply = response.content
        else:
            reply = str(response)

        print(f"\nâœ… RESPOSTA RECEBIDA:")
        print(f"{'=' * 80}")
        print(reply)
        print(f"{'=' * 80}")

        return True

    except Exception as e:
        print(f"\nâŒ ERRO: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_multi_provider_streaming():
    """Testa streaming do MultiProviderLLM."""
    print("\n" + "=" * 80)
    print("ğŸ“‹ TESTE 2: MultiProviderLLM com Streaming")
    print("=" * 80)

    llm = MultiProviderLLM(enable_streaming=True)

    print(f"\nâœ… Streaming habilitado")
    print(f"ğŸ“ Provedor primÃ¡rio: {llm.get_current_provider()}")

    messages = [
        {"role": "system", "content": "VocÃª Ã© um assistente que conta atÃ© 5."},
        {"role": "user", "content": "Conte de 1 atÃ© 5, um nÃºmero por linha."},
    ]

    print(f"\nğŸ’¬ Enviando mensagem para streaming...")
    print(f"\nâœ… RESPOSTA (STREAMING):")
    print(f"{'=' * 80}")

    try:
        full_response = ""
        async for chunk in llm.astream(messages):
            print(chunk, end="", flush=True)
            full_response += chunk

        print(f"\n{'=' * 80}")
        print(f"\nâœ… Stream concluÃ­do! Total de caracteres: {len(full_response)}")

        return True

    except Exception as e:
        print(f"\nâŒ ERRO: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_intake_agent_with_multi_provider():
    """Testa IntakeAgent com o sistema multi-provedor."""
    print("\n" + "=" * 80)
    print("ğŸ“‹ TESTE 3: IntakeAgent com Multi-Provider")
    print("=" * 80)

    agent = IntakeAgent()

    print(f"\nâœ… Agente criado: {agent.id}")
    print(f"ğŸ“ Nome: {agent.name}")
    print(f"ğŸ“„ DescriÃ§Ã£o: {agent.description}")

    # Verifica se estÃ¡ usando MultiProviderLLM
    print(f"\nğŸ” Verificando tipo do LLM...")
    print(f"   Tipo: {type(agent.llm).__name__}")
    print(f"   Provedores disponÃ­veis: {agent.llm.get_available_providers()}")

    # Testa conversaÃ§Ã£o
    print(f"\nğŸ’¬ Testando conversaÃ§Ã£o de triagem...")

    message = "OlÃ¡, bom dia! Gostaria de agendar uma consulta"
    context = {
        "user_id": "patient_001",
        "session_id": "session_001",
        "metadata": {"source": "whatsapp", "phone": "+5511987654321"},
    }

    print(f"\nğŸ“¤ Mensagem do paciente:")
    print(f"   '{message}'")
    print(f"\nğŸ“‹ Contexto:")
    print(f"   - user_id: {context['user_id']}")
    print(f"   - source: {context['metadata']['source']}")
    print(f"   - phone: {context['metadata']['phone']}")

    try:
        reply = await agent.run(message=message, context=context)

        print(f"\nâœ… RESPOSTA DO AGENTE:")
        print(f"{'=' * 80}")
        print(reply)
        print(f"{'=' * 80}")

        return True

    except Exception as e:
        print(f"\nâŒ ERRO: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_fallback_mechanism():
    """Testa o mecanismo de fallback entre provedores."""
    print("\n" + "=" * 80)
    print("ğŸ“‹ TESTE 4: Mecanismo de Fallback")
    print("=" * 80)

    print(f"\nğŸ“ Neste teste, vamos tentar invocar o LLM.")
    print(f"   Se Groq falhar (403), deve tentar OpenAI.")
    print(f"   Se OpenAI falhar, deve usar FakeModel.")
    print(f"\nğŸ”„ Iniciando teste de fallback...")

    llm = MultiProviderLLM(enable_streaming=False)

    messages = [{"role": "user", "content": "Teste de fallback"}]

    try:
        response = await llm.ainvoke(messages)

        if hasattr(response, "content"):
            reply = response.content
        else:
            reply = str(response)

        print(f"\nâœ… Sistema de fallback funcionou!")
        print(f"ğŸ“¤ Resposta recebida: {reply[:100]}...")

        return True

    except Exception as e:
        print(f"\nâŒ Todos os provedores falharam: {e}")
        return False


async def main():
    """Executa todos os testes."""
    print("\nğŸš€ Iniciando bateria de testes...\n")

    results = {}

    # Teste 1: MultiProviderLLM direto
    results["test1"] = await test_multi_provider_direct()

    # Teste 2: Streaming
    results["test2"] = await test_multi_provider_streaming()

    # Teste 3: IntakeAgent
    results["test3"] = await test_intake_agent_with_multi_provider()

    # Teste 4: Fallback
    results["test4"] = await test_fallback_mechanism()

    # Resumo
    print("\n" + "=" * 80)
    print("ğŸ“Š RESUMO DOS TESTES")
    print("=" * 80)

    total = len(results)
    passed = sum(1 for v in results.values() if v)

    print(f"\nâœ… Testes passaram: {passed}/{total}")
    print(f"\nDetalhes:")
    print(f"  {'âœ…' if results.get('test1') else 'âŒ'} Teste 1: MultiProviderLLM Direto")
    print(f"  {'âœ…' if results.get('test2') else 'âŒ'} Teste 2: Streaming")
    print(f"  {'âœ…' if results.get('test3') else 'âŒ'} Teste 3: IntakeAgent")
    print(f"  {'âœ…' if results.get('test4') else 'âŒ'} Teste 4: Fallback Mechanism")

    if passed == total:
        print(f"\nğŸ‰ TODOS OS TESTES PASSARAM!")
        print(f"\nâœ… Sistema multi-provedor configurado com sucesso:")
        print(f"   - Groq como primÃ¡rio")
        print(f"   - OpenAI como fallback")
        print(f"   - FakeModel como Ãºltimo recurso")
        print(f"   - Streaming habilitado")
    else:
        print(f"\nâš ï¸  Alguns testes falharam. Verifique os logs acima.")

    print("=" * 80)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Testes interrompidos pelo usuÃ¡rio")
    except Exception as e:
        print(f"\n\nâŒ Erro fatal: {e}")
        import traceback

        traceback.print_exc()
