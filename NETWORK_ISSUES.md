# üåê Problemas de Rede Identificados

## Situa√ß√£o Atual

O ambiente de desenvolvimento est√° atr√°s de um **proxy/firewall** que bloqueia requisi√ß√µes HTTPS para APIs externas de LLM.

## üîç Evid√™ncias

### 1. Google Gemini
```
Erro: HTTP 403 Forbidden
IP resolvido: 21.0.0.13 (IP de proxy, n√£o Google)
SSL Error: CERTIFICATE_VERIFY_FAILED (self signed certificate)
```

### 2. Groq
```
Erro: HTTP 403 "Access denied"
Causa: Problema de billing/quota (n√£o √© rede)
Pode funcionar ap√≥s resolver no console
```

## üéØ Solu√ß√µes Poss√≠veis

### Op√ß√£o 1: FakeModel (ATUAL) ‚úÖ
```bash
USE_FAKE_MODEL=true
DEFAULT_MODEL=fake
```
**Vantagens:**
- ‚úÖ Funciona sem restri√ß√µes
- ‚úÖ Valida toda a l√≥gica
- ‚úÖ Sem depend√™ncia de rede

**Limita√ß√µes:**
- ‚ö†Ô∏è  Respostas sempre iguais
- ‚ö†Ô∏è  N√£o testa modelo real

---

### Op√ß√£o 2: Ollama (LOCAL) ‚≠ê RECOMENDADO
```bash
# Instalar Ollama localmente
curl -fsSL https://ollama.com/install.sh | sh

# Baixar modelo
ollama pull llama3.2

# Iniciar servidor
ollama serve

# Configurar .env
OLLAMA_MODEL=llama3.2
DEFAULT_MODEL=ollama
```

**Vantagens:**
- ‚úÖ Roda 100% local (sem internet)
- ‚úÖ Sem limites de uso
- ‚úÖ Sem bloqueios de firewall
- ‚úÖ Modelos reais (llama3, mistral, etc)
- ‚úÖ Respostas variadas e inteligentes

**Requisitos:**
- RAM: 8GB+ recomendado
- CPU: Qualquer (GPU opcional)
- Espa√ßo: ~4-8GB por modelo

---

### Op√ß√£o 3: Resolver Proxy/Firewall

Se voc√™ tem acesso administrativo ao ambiente:

```bash
# Configurar proxy bypass
export NO_PROXY="localhost,127.0.0.1,*.googleapis.com,*.groq.com"

# OU desabilitar verifica√ß√£o SSL (n√£o recomendado em produ√ß√£o)
export CURL_CA_BUNDLE=""
export REQUESTS_CA_BUNDLE=""
```

‚ö†Ô∏è **N√£o recomendado:** Desabilitar verifica√ß√£o SSL √© um risco de seguran√ßa.

---

### Op√ß√£o 4: Usar VPN/Tunnel

Se permitido pela pol√≠tica da organiza√ß√£o:

```bash
# SSH Tunnel
ssh -D 8080 usuario@servidor-externo

# Configurar proxy SOCKS
export ALL_PROXY=socks5://localhost:8080
```

---

## üí° Recomenda√ß√£o para Produ√ß√£o

### Para Desenvolvimento (Agora):
```bash
# Use Ollama - funciona localmente
OLLAMA_MODEL=llama3.2
DEFAULT_MODEL=ollama
```

### Para Produ√ß√£o (Quando deploy):
```bash
# Use Groq (ap√≥s resolver billing) ou Gemini
# Em servidor de produ√ß√£o sem proxy
GROQ_API_KEY=gsk_...
DEFAULT_MODEL=llama-3.1-8b
```

---

## üß™ Como Testar

### Teste de conectividade:
```bash
# Testa se consegue acessar APIs
curl -I https://generativelanguage.googleapis.com
curl -I https://api.groq.com

# Se der erro SSL ou proxy, est√° bloqueado
```

### Teste com Ollama:
```bash
# Instala e testa
ollama pull llama3.2
ollama run llama3.2 "Diga ol√° em portugu√™s"

# Se funcionar, configure no .env
```

---

## üìä Compara√ß√£o de Op√ß√µes

| Op√ß√£o | Funciona? | Custo | Qualidade | Setup |
|-------|-----------|-------|-----------|-------|
| FakeModel | ‚úÖ Sim | Gr√°tis | ‚≠ê | 0min |
| **Ollama** | ‚úÖ Sim | Gr√°tis | ‚≠ê‚≠ê‚≠ê‚≠ê | 5min |
| Gemini | ‚ùå Bloqueado | Gr√°tis | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | - |
| Groq | ‚ùå Billing | Gr√°tis | ‚≠ê‚≠ê‚≠ê‚≠ê | - |

---

## üöÄ Pr√≥ximos Passos

1. **Continue desenvolvimento com FakeModel**
   - Valida toda a l√≥gica
   - Implementa features
   - Testes estruturais

2. **Instale Ollama quando quiser testar modelo real**
   - Sem bloqueios
   - Sem custos
   - Respostas reais

3. **Em produ√ß√£o, use Groq ou Gemini**
   - Ap√≥s resolver problemas de rede/billing
   - APIs externas funcionar√£o normalmente
